{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (2.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aakas\\anaconda3\\envs\\ift511\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Data\n",
    "train_df = pd.read_csv('train (1).csv')\n",
    "test_df = pd.read_csv('test (1).csv')\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Begin Data Preprocessing\n",
    "\n",
    "# Drop non-essential columns\n",
    "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\519829561.py:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Extract Title from Name\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate and replace rare titles\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Title to a numerical feature\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original 'Name' column and 'PassengerId' from training set\n",
    "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Sex' to a numerical feature\n",
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing 'Age' values based on Pclass and Sex\n",
    "guess_ages = np.zeros((2,3))\n",
    "for dataset in combine:\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n",
    "            age_guess = guess_df.median()\n",
    "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1), 'Age'] = guess_ages[i,j]\n",
    "    dataset['Age'] = dataset['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin 'Age' into ordinal categories (Corrected from original notebook)\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create 'FamilySize' and 'IsAlone'\n",
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original family-related columns\n",
    "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create 'Age*Class'\n",
    "for dataset in combine:\n",
    "    dataset['Age*Class'] = dataset.Age * dataset.Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing 'Embarked' values with the most frequent port\n",
    "freq_port = train_df.Embarked.dropna().mode()[0]\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Embarked' to a numerical feature\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\2836775784.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Impute missing 'Fare' in test set with the median\n",
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin 'Fare' into ordinal categories\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare Data for Modeling\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train and Evaluate Models\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred_log = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred_svc = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred_gaussian = gaussian.predict(X_test)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred_perceptron = perceptron.predict(X_test)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "linear_svc = LinearSVC(dual=False) # dual=False to avoid ConvergenceWarning\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred_linear_svc = linear_svc.predict(X_test)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred_sgd = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred_dt = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred_rf = random_forest.predict(X_test)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Accuracy on Training Data ---\n",
      "                        Model  Score\n",
      "3               Random Forest  86.64\n",
      "8               Decision Tree  86.64\n",
      "0     Support Vector Machines  82.83\n",
      "1                         KNN  82.83\n",
      "2         Logistic Regression  81.37\n",
      "7                  Linear SVC  79.46\n",
      "5                  Perceptron  79.35\n",
      "4                 Naive Bayes  76.88\n",
      "6  Stochastic Gradient Decent  74.97\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Consolidate Model Scores\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "print(\"--- Model Accuracy on Training Data ---\")\n",
    "print(models.sort_values(by='Score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Improved Model Accuracies on Training Data ---\n",
      "                        Model  Score\n",
      "7               Decision Tree  98.43\n",
      "8               Random Forest  98.43\n",
      "2                         KNN  87.54\n",
      "1     Support Vector Machines  83.16\n",
      "0         Logistic Regression  82.15\n",
      "5                  Linear SVC  81.37\n",
      "6  Stochastic Gradient Decent  79.91\n",
      "3                 Naive Bayes  78.11\n",
      "4                  Perceptron  76.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\1686104532.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df['Embarked'].fillna(combined_df['Embarked'].mode()[0], inplace=True)\n",
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\1686104532.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df['Fare'].fillna(combined_df['Fare'].median(), inplace=True)\n",
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\1686104532.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_4968\\1686104532.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Step 1: Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv('train (1).csv')\n",
    "    test_df = pd.read_csv('test (1).csv')\n",
    "    \n",
    "    # Store test PassengerId for submission file\n",
    "    test_passenger_id = test_df['PassengerId']\n",
    "    \n",
    "    # Combine train and test sets for consistent processing\n",
    "    combined_df = pd.concat([train_df.drop('Survived', axis=1), test_df], ignore_index=True)\n",
    "    \n",
    "    # --- Step 2: Feature Engineering & Imputation ---\n",
    "    \n",
    "    \n",
    "    # Create 'Title' feature\n",
    "    combined_df['Title'] = combined_df.Name.str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "    combined_df['Title'] = combined_df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    combined_df['Title'] = combined_df['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n",
    "    \n",
    "    # Create 'IsAlone' feature\n",
    "    combined_df['FamilySize'] = combined_df['SibSp'] + combined_df['Parch'] + 1\n",
    "    combined_df['IsAlone'] = 0\n",
    "    combined_df.loc[combined_df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    combined_df.drop(columns=['SibSp', 'Parch', 'FamilySize'], inplace=True)\n",
    "    \n",
    "    # Impute missing values\n",
    "    combined_df['Embarked'].fillna(combined_df['Embarked'].mode()[0], inplace=True)\n",
    "    combined_df['Fare'].fillna(combined_df['Fare'].median(), inplace=True)\n",
    "    combined_df['Age'] = combined_df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # --- Step 3: One-Hot Encode Categorical Features (Improvement) ---\n",
    "    categorical_features = ['Embarked', 'Title', 'Sex']\n",
    "    combined_df = pd.get_dummies(combined_df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Drop columns that are not useful\n",
    "    combined_df.drop(columns=['Ticket', 'Cabin', 'Name', 'PassengerId'], inplace=True)\n",
    "    \n",
    "    # --- Step 4: Split back into training and testing sets ---\n",
    "    X_train = combined_df.iloc[:len(train_df)]\n",
    "    X_test = combined_df.iloc[len(train_df):]\n",
    "    Y_train = train_df['Survived']\n",
    "\n",
    "    # --- Step 5: Scale Numerical Features (Improvement) ---\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = ['Age', 'Fare', 'Pclass']\n",
    "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "    \n",
    "    # Save the processed files\n",
    "    X_train.to_csv(\"improved_train.csv\", index=False)\n",
    "    X_test.to_csv(\"improved_test.csv\", index=False)\n",
    "    \n",
    "    # --- Step 6: Train and Evaluate Models on Improved Data ---\n",
    "    models_data = []\n",
    "    \n",
    "    # Define models\n",
    "    models_to_run = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Support Vector Machines\": SVC(),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"Linear SVC\": LinearSVC(dual=False, max_iter=2000),\n",
    "        \"Stochastic Gradient Decent\": SGDClassifier(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100)\n",
    "    }\n",
    "\n",
    "    # Train and score models\n",
    "    for name, model in models_to_run.items():\n",
    "        model.fit(X_train, Y_train)\n",
    "        score = round(model.score(X_train, Y_train) * 100, 2)\n",
    "        models_data.append({'Model': name, 'Score': score})\n",
    "    \n",
    "    # --- Step 7: Display Improved Accuracies ---\n",
    "    improved_models_df = pd.DataFrame(models_data)\n",
    "    print(\"--- Improved Model Accuracies on Training Data ---\")\n",
    "    print(improved_models_df.sort_values(by='Score', ascending=False))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Execution halted. Please upload 'train (1).csv' and 'test (1).csv' to proceed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFT511",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
